---
title: "Parameter Fitting with Synthetic Datasets - All Variants"
author: "Omid Arhami"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parameter Fitting with Synthetic Datasets - All Variants}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 6,
  warning = TRUE,
 message = TRUE,    # Show messages
  error = TRUE      # Continue on error
)



#' Filter matrix to only virus vs antiserum distances
#'
#' @param dist_matrix Distance matrix
#' @param selected_names Names of selected reference points
#' @return A `matrix` of the same dimensions as the input, but with non-virus-vs-antiserum distances set to `NA`.
only_virus_vs_as <- function(dist_matrix, selected_names) {
  if (!is.matrix(dist_matrix)) {
    stop("dist_matrix must be a matrix")
  }
  if (!is.character(selected_names)) {
    stop("selected_names must be a character vector")
  }
  
  # Make values NA for non-selected columns
  dist_matrix[, !colnames(dist_matrix) %in% selected_names] <- NA
  
  # Update symmetric values
  dist_matrix[rownames(dist_matrix) %in% selected_names, ] <- 
    t(dist_matrix[, colnames(dist_matrix) %in% selected_names])
  
  return(dist_matrix)
}


#' Generate unique string identifiers with year suffix
#'
#' @param n Number of strings to generate
#' @param length Length of random part of string (default: 8)
#' @param lower_bound Lower bound for year suffix (default: 1)
#' @param upper_bound Upper bound for year suffix (default: 20)
#' @return A character vector of unique strings with year suffixes
generate_unique_string <- function(n, length = 8, lower_bound = 1, upper_bound = 20) {
  if (n <= 0) stop("n must be positive")
  if (length <= 0) stop("length must be positive")
  if (lower_bound > upper_bound) {
    stop("lower_bound must be less than or equal to upper_bound")
  }
  
  # Validate numeric parameters
  if (!all(sapply(list(n, length, lower_bound, upper_bound), is.numeric))) {
    stop("All numeric parameters must be numeric")
  }
  
  if (!all(sapply(list(n, length), function(x) x == round(x)))) {
    stop("n and length must be integers")
  }

  # Generate base strings
  base_strings <- sapply(1:n, function(x) {
    paste0(sample(c(LETTERS, 0:9), length, replace = TRUE), collapse = "")
  })
  
  # Calculate strings per year
  num_per_group <- n / (upper_bound - lower_bound + 1)
  
  # Generate year suffixes
  suffixes <- rep(lower_bound:upper_bound, each = ceiling(num_per_group))[1:n]
  
  # Combine strings with suffixes
  unique_strings <- paste0(base_strings, "/", suffixes)
  
  return(unique_strings)
}



#' Generate Synthetic Distance Matrices with Missing Data
#'
#' @description
#' Creates synthetic distance matrices with controlled levels of missingness and noise
#' for testing and validating mapping algorithms. Generates multiple datasets with 
#' different dimensionalities and missingness patterns. If `output_dir` is provided,
#' the generated datasets are saved as RDS files.
#'
#' @param n_dims_list Numeric vector of dimensions to generate data for
#' @param seeds Integer vector of random seeds (same length as n_dims_list)
#' @param n_points Integer number of points to generate
#' @param missingness_levels Named list of missingness percentages (default: list(S=0.67, M=0.77, L=0.87))
#' @param output_dir Character path to directory for saving outputs. If NULL (the default), no files are saved.
#' @param prefix Character string to prefix output files (optional)
#' @param save_plots Logical whether to save network visualization plots. Requires `output_dir` to be set.
#' @return A list containing the generated synthetic data and metadata:
#'   \item{matrices}{A list of generated symmetric distance matrices for each dimension.}
#'   \item{panels}{A list of generated assay panels (non-symmetric matrices) for each dimension.}
#'   \item{metadata}{A `data.frame` with the generation parameters for each dataset.}
#' @examples
#' # Generate datasets without saving to disk
#' results <- generate_synthetic_datasets(
#'   n_dims_list = c(2, 3),
#'   seeds = c(123, 456),
#'   n_points = 50
#' )
#' \donttest{
#' # Generate datasets and save to a temporary directory
#' temp_out_dir <- tempdir()
#' results_saved <- generate_synthetic_datasets(
#'   n_dims_list = c(2),
#'   seeds = c(123),
#'   n_points = 10,
#'   missingness_levels = list(low=0.5, high=0.8),
#'   output_dir = temp_out_dir,
#'   save_plots = TRUE
#' )
#' list.files(temp_out_dir)
#' # Clean up the directory
#' unlink(temp_out_dir, recursive = TRUE)
#' }
#' @importFrom stats dist
generate_synthetic_datasets <- function(n_dims_list, seeds, n_points,
                                     missingness_levels = list(S=0.67, M=0.77, L=0.87),
                                     output_dir = NULL,
                                     prefix = "sim",
                                     save_plots = FALSE) {
  
  # Input validation
  if (length(n_dims_list) != length(seeds)) {
    stop("n_dims_list and seeds must have the same length")
  }
  
  if (save_plots && is.null(output_dir)) {
    stop("An 'output_dir' must be provided when 'save_plots' is TRUE.", call. = FALSE)
  }
  
  if (!is.null(output_dir)) {
    if (!dir.exists(output_dir)) {
      dir.create(output_dir, recursive = TRUE)
    }
  }
  
  # Validate dimensions
  if (!is.numeric(n_dims_list) || any(n_dims_list < 1) || 
      any(n_dims_list != round(n_dims_list))) {
    stop("All dimensions must be positive integers")
  }
  
  # Validate seeds
  if (!is.numeric(seeds) || any(seeds != round(seeds))) {
    stop("All seeds must be integers")
  }
  
  # Validate n_points
  if (!is.numeric(n_points) || n_points < 2 || n_points != round(n_points)) {
    stop("n_points must be an integer >= 2")
  }
  
  # Validate missingness levels
  if (!is.list(missingness_levels)) {
    stop("missingness_levels must be a list")
  }
  
  if (!all(unlist(missingness_levels) > 0 & unlist(missingness_levels) < 1)) {
    stop("All missingness levels must be between 0 and 1")
  }
  
  # Initialize return lists
  coordinates_list <- list()
  matrices_list <- list()
  panels_list <- list()
  metadata <- data.frame()
  
  # Process each dimension
  for (i in seq_along(n_dims_list)) {
    set.seed(seeds[i])
    n_dims <- n_dims_list[i]
    
    # Generate base dataset
    dataset <- generate_complex_data(n_points, n_dims, 
                                   n_clusters = 5, 
                                   cluster_spread = 0.1)
    
    # Add unique IDs
    dataset$unique_id <- generate_unique_string(nrow(dataset))
    
    # Generate distance matrix
    dist_matrix <- as.matrix(dist(dataset[, 1:n_dims]))
    rownames(dist_matrix) <- dataset$unique_id
    colnames(dist_matrix) <- dataset$unique_id
    
    # Select references
    n_ref <- floor(0.4 * n_points)
    selected_indices <- floor(seq(1, nrow(dataset), length.out = n_ref + 1))
    selected_indices <- selected_indices[1:n_ref]
    selected_names <- dataset$unique_id[selected_indices]
    
    # Label references and viruses
    virus_mask <- !(rownames(dist_matrix) %in% selected_names)
    dataset$unique_id[virus_mask] <- paste0("V/", dataset$unique_id[virus_mask])
    rownames(dist_matrix)[virus_mask] <- paste0("V/", rownames(dist_matrix)[virus_mask])
    colnames(dist_matrix)[virus_mask] <- paste0("V/", colnames(dist_matrix)[virus_mask])
    
    serum_mask <- rownames(dist_matrix) %in% selected_names
    dataset$unique_id[serum_mask] <- paste0("S/", dataset$unique_id[serum_mask])
    rownames(dist_matrix)[serum_mask] <- paste0("S/", rownames(dist_matrix)[serum_mask])
    colnames(dist_matrix)[serum_mask] <- paste0("S/", colnames(dist_matrix)[serum_mask])
    
    selected_names <- paste0("S/", selected_names)
    
    # Save the coordinates to return
    coordinates_list[[i]] <- dataset
    
    # Create base matrices
    matrices <- list()
    matrices$complete <- dist_matrix
    matrices$labelled <- only_virus_vs_as(dist_matrix, selected_names)
    
    # Generate matrices with different missingness levels
    for (level_name in names(missingness_levels)) {
      level_pct <- missingness_levels[[level_name]]
      missing_mat <- increase_na_percentage(matrices$labelled, level_pct)
      matrices[[paste0(level_name, "_missing")]] <- missing_mat
      
      # Add noise variations
      noisy <- add_noise_bias(missing_mat)
      matrices[[paste0(level_name, "_noise1")]] <- noisy$n1
      matrices[[paste0(level_name, "_noise2")]] <- noisy$n2
      matrices[[paste0(level_name, "_noise_bias")]] <- noisy$nb
    }
    
    # Generate panels
    panels <- list()
    panels$full <- symmetric_to_nonsymmetric_matrix(matrices$labelled, selected_names)
    
    for (level_name in names(missingness_levels)) {
      missing_mat <- matrices[[paste0(level_name, "_missing")]]
      panel <- symmetric_to_nonsymmetric_matrix(missing_mat, selected_names)
      panel <- panel[rowSums(!is.na(panel)) > 0, ]
      panels[[level_name]] <- panel
    }
    
    # Store results
    matrices_list[[i]] <- matrices
    panels_list[[i]] <- panels
    
    # Record metadata
    metadata <- rbind(metadata, data.frame(
      dimension = n_dims,
      seed = seeds[i],
      n_points = n_points,
      n_references = n_ref
    ))
    
    # Save files if output directory provided
    if (!is.null(output_dir)) {
      base_name <- file.path(output_dir, 
                            sprintf("%s_dim%d", prefix, n_dims))
      
      # Save matrices
      for (mat_name in names(matrices)) {
        saveRDS(matrices[[mat_name]], 
                paste0(base_name, "_", mat_name, ".rds"))
      }
      
      # Save panels
      for (panel_name in names(panels)) {
        saveRDS(panels[[panel_name]], 
                paste0(base_name, "_panel_", panel_name, ".rds"))
      }
      
      # Create network plots if requested
      if (save_plots) {
        for (level_name in names(missingness_levels)) {
          mat <- matrices[[paste0(level_name, "_missing")]]
          net <- analyze_network_structure(mat)
          plot_file_path <- paste0(base_name, "_network_", level_name, ".png")
          plot_network_structure(net, output_file = plot_file_path)
        }
      }
    }
  }
  
  # Return results
  return(list(
    coordinates = coordinates_list,
    matrices = matrices_list,
    panels = panels_list,
    metadata = metadata
  ))
}



#' Generate Complex High-Dimensional Data for Testing
#'
#' @description 
#' Generates synthetic high-dimensional data with clusters and trends for testing 
#' dimensionality reduction methods. Creates data with specified properties:
#' - Multiple clusters along a trend line
#' - Variable density regions 
#' - Controllable noise levels
#' - Optional visualization
#'
#' The function generates cluster centers along a trend line, adds points around those
#' centers with specified spread, and incorporates random noise to create high and 
#' low density areas. The data is useful for testing dimensionality reduction and
#' visualization methods.
#'
#' @param n_points Integer number of points to generate
#' @param n_dim Integer number of dimensions 
#' @param n_clusters Integer number of clusters
#' @param cluster_spread Numeric controlling cluster variance
#' @param fig_name Character path to save visualization (optional)
#' @return A `data.frame` with `n_points` rows and `n_dim` columns. Column names 
#'         are "Dim1" through "DimN" where N is n_dim.
#' @examples
#' # Generate basic dataset
#' data <- generate_complex_data(n_points = 500, n_dim = 10, 
#'                              n_clusters = 4, cluster_spread = 1)
#'                              
#' # The function returns a data frame, which can be inspected
#' head(data)
#'
#' @importFrom MASS mvrnorm
#' @importFrom stats runif rnorm prcomp
#' @importFrom ggplot2 ggplot aes geom_point theme_minimal labs coord_fixed ggsave
generate_complex_data <- function(n_points = 500, n_dim = 10, n_clusters = 4, 
                                cluster_spread = 1, fig_name = NA) {
  # Input validation
  if (!is.numeric(n_points) || n_points <= 0 || n_points %% 1 != 0) {
    stop("n_points must be a positive integer")
  }
  if (!is.numeric(n_dim) || n_dim <= 0 || n_dim %% 1 != 0) {
    stop("n_dim must be a positive integer")
  }
  if (!is.numeric(n_clusters) || n_clusters <= 0 || n_clusters %% 1 != 0) {
    stop("n_clusters must be a positive integer")
  }
  if (!is.numeric(cluster_spread) || cluster_spread <= 0) {
    stop("cluster_spread must be a positive number")
  }
  if (!is.na(fig_name) && !is.character(fig_name)) {
    stop("fig_name must be NA or a character string ending in .png")
  }
  
  if (n_clusters > n_points) {
    stop("Number of clusters cannot exceed number of points")
  }
  
  # Generate a trend vector
  trend <- seq(-10, 10, length.out = n_points)
  
  # Generate cluster centers along the trend
  cluster_centers <- matrix(0, nrow = n_clusters, ncol = n_dim)
  for (i in 1:n_clusters) {
    cluster_centers[i, ] <- 0.5 * runif(n_dim, min = -10, max = 10) + 
      trend[floor(i * n_points / n_clusters)]
  }
  
  # Generate points around each cluster center
  points_per_cluster <- floor(n_points / n_clusters)
  data <- do.call(rbind, lapply(1:n_clusters, function(i) {
    mvrnorm(n = points_per_cluster, 
            mu = cluster_centers[i, ], 
            Sigma = diag(cluster_spread, n_dim))
  }))
  
  # Add random noise to create high and low density areas
  noise <- matrix(rnorm(nrow(data) * n_dim, mean = 0, sd = 3.3), 
                 ncol = n_dim)
  data <- data + noise
  
  # Add trend to multiple dimensions
  for (j in 1:n_dim) {
    data[, j] <- data[, j] + trend[1:nrow(data)]
  }
  
  # Convert to data frame with proper column names
  data <- as.data.frame(data)
  colnames(data) <- paste0("Dim", 1:n_dim)
  
  # Create visualization if requested
  if (!is.na(fig_name)) {
    # Perform PCA for visualization
    pca_result <- prcomp(data, scale. = FALSE)
    pca_data <- as.data.frame(pca_result$x[, 1:2])
    colnames(pca_data) <- c("PC1", "PC2")
    
    # Create plot
    p <- ggplot(pca_data, aes(x = .data$PC1, y = .data$PC2)) +
      geom_point(alpha = 0.5) +
      theme_minimal() +
      labs(title = "PCA of Simulated High-Dimensional Data",
           x = "PC 1", y = "PC 2") +
      coord_fixed()
    
    # ENSURE DIRECTORY EXISTS
    output_dir <- dirname(fig_name)
    if (!dir.exists(output_dir)) {
        dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
    }

    # Save plot
    ggsave_white_bg(filename = fig_name, plot = p, dpi = 300)
  }
  
  return(data)
}



#' Increase Missing Values in a Matrix
#'
#' @description
#' Strategically introduces NA values into a distance matrix while maintaining symmetry.
#' New NA values are added preferentially farther from the diagonal to simulate real-world 
#' measurement patterns where distant pairs are more likely to be unmeasured.
#'
#' @details
#' The function:
#' 1. Calculates needed additional NAs to reach target percentage
#' 2. Creates probability matrix favoring off-diagonal elements
#' 3. Randomly selects positions weighted by distance from diagonal
#' 4. Maintains matrix symmetry by mirroring NAs
#'
#' @param mat Matrix to modify
#' @param target_na_percentage Numeric between 0 and 1 specifying desired proportion of NAs
#' @return A `matrix` with an increased number of `NA` values, maintaining symmetry.
#' @examples
#' # Create sample distance matrix
#' dist_mat <- matrix(runif(100), 10, 10)
#' dist_mat[lower.tri(dist_mat)] <- t(dist_mat)[lower.tri(dist_mat)]
#' diag(dist_mat) <- 0
#' 
#' # Increase NAs to 70%
#' sparse_mat <- increase_na_percentage(dist_mat, 0.7)
increase_na_percentage <- function(mat, target_na_percentage) {
  # Input validation
  if (!is.matrix(mat)) {
    stop("Input must be a matrix")
  }
  if (!is.numeric(target_na_percentage) || 
      target_na_percentage <= 0 || 
      target_na_percentage >= 1) {
    stop("target_na_percentage must be between 0 and 1")
  }
  
  # Calculate current and target NA counts
  total_elements <- length(mat)
  current_na_count <- sum(is.na(mat))
  target_na_count <- ceiling(target_na_percentage * total_elements)
  additional_na_count <- target_na_count - current_na_count
  
  if (additional_na_count <= 0) {
    warning("The matrix already has more NA values than the target percentage.")
    return(mat)
  }
  
  # Get indices of non-NA elements
  non_na_indices <- which(!is.na(mat))
  
  # Calculate distance from diagonal for each element
  n <- nrow(mat)
  distances <- outer(1:n, 1:n, FUN = function(i, j) abs(i - j))
  
  # Create probability matrix favoring off-diagonal elements
  max_distance <- max(distances)
  # Add padding to avoid strictly diagonal matrix
  distances <- distances + 0.15 * max_distance
  
  # Normalize probabilities
  prob_matrix <- distances / max_distance
  prob_vector <- as.vector(prob_matrix)
  prob_vector <- prob_vector / sum(prob_vector)
  
  # Randomly sample indices based on probability vector
  random_indices <- sample(non_na_indices, 
                         size = additional_na_count,
                         prob = prob_vector[non_na_indices], 
                         replace = FALSE)
  
  # Replace selected elements with NA while maintaining symmetry
  for (index in random_indices) {
    row <- (index - 1) %/% nrow(mat) + 1
    col <- (index - 1) %% ncol(mat) + 1
    if (row != col && !is.na(mat[row, col])) {
      mat[row, col] <- NA
      mat[col, row] <- NA
    }
  }
  
  return(mat)
}


#' Add Noise and Bias to Matrix Data
#'
#' @description
#' Creates noisy versions of a distance matrix by adding random noise and/or systematic bias.
#' Useful for testing robustness of algorithms to measurement errors and systematic biases.
#'
#' @details
#' The function generates three variants of the input matrix:
#' 1. n1: Matrix with random Gaussian noise
#' 2. n2: Different realization of random noise
#' 3. nb: Matrix with both random noise and systematic negative bias
#'
#' The noise level is scaled relative to the data mean to maintain realistic error magnitudes.
#'
#' @param matrix_data Numeric matrix to add noise to
#' @return A `list` containing three noisy `matrix` objects:
#'   \item{n1}{Matrix with the first realization of random Gaussian noise.}
#'   \item{n2}{Matrix with a second, different realization of random Gaussian noise.}
#'   \item{nb}{Matrix with both random noise and a systematic negative bias.}
#' @examples
#' # Create sample distance matrix
#' dist_mat <- matrix(runif(100), 10, 10)
#' dist_mat[lower.tri(dist_mat)] <- t(dist_mat)[lower.tri(dist_mat)]
#' diag(dist_mat) <- 0
#'
#' # Generate noisy versions
#' noisy_variants <- add_noise_bias(dist_mat)
#' @importFrom stats rnorm
add_noise_bias <- function(matrix_data) {
  # Input validation
  if (!is.matrix(matrix_data) || !is.numeric(matrix_data)) {
    stop("Input must be a numeric matrix")
  }
  
  # Calculate noise scale based on data mean
  data_mean <- mean(matrix_data, na.rm = TRUE)
  measurement_er <- 0.05 * data_mean
  
  # Generate first noisy variant
  noise1 <- matrix(rnorm(length(matrix_data), 
                        mean = 0, 
                        sd = measurement_er), 
                  nrow = nrow(matrix_data), 
                  ncol = ncol(matrix_data))
  noisy_matrix1 <- matrix_data + noise1
  
  # Enforce non-negative values and zero diagonal
  noisy_matrix1[noisy_matrix1 < 0] <- 0
  diag(noisy_matrix1) <- 0
  
  # Generate second noisy variant
  noise2 <- matrix(rnorm(length(matrix_data), 
                        mean = 0, 
                        sd = measurement_er),
                  nrow = nrow(matrix_data), 
                  ncol = ncol(matrix_data))
  noisy_matrix2 <- matrix_data + noise2
  noisy_matrix2[noisy_matrix2 < 0] <- 0
  diag(noisy_matrix2) <- 0
  
  # Generate biased variant
  bias <- matrix(measurement_er, 
                nrow = nrow(matrix_data), 
                ncol = ncol(matrix_data))
  noisy_biased_matrix <- matrix_data + noise2 - bias
  noisy_biased_matrix[noisy_biased_matrix < 0] <- 0
  diag(noisy_biased_matrix) <- 0

  # Return all variants
  return(list(
    n1 = noisy_matrix1,
    n2 = noisy_matrix2,
    nb = noisy_biased_matrix
  ))
}

```

## Introduction

This vignette demonstrates parameter fitting for synthetic datasets across different:
* Dimensionalities (2D, 5D, 10D)
* Missing data patterns (S, M, L levels)
* Noise types (random noise & biased noise)

There are 27 combinations. Therefore, to run the simulation in this script, local parallel processing will be used.

## Required Packages

```{r}
library(topolow)
library(ggplot2)
library(dplyr)
#library(data.table)
library(gridExtra)
```

## Generate Synthetic Datasets

First, let's create synthetic datasets with different dimensionalities:

```{r}
# Generate datasets in 2, 5, and 10 dimensions
results <- generate_synthetic_datasets(
  n_dims_list = c(2, 5, 10),
  seeds = c(123450, 903450, 13450),
  n_points = 250,
  output_dir = "synthetic_data"
)

# Print dataset properties
print(results$metadata)

# List available matrix variants for each dimension
matrix_variants <- names(results$matrices[[1]])[c(3,4,6,7,8,10,11,12,14)]
print("Available matrix variants:")
print(matrix_variants)
```

## Parameter Optimization Setup

Define consistent parameter ranges and optimization settings:

```{r}
# Define parameter ranges
param_ranges <- list(
  N_min = 2,          # Minimum dimensions 
  N_max = 30,         # Maximum dimensions
  k0_min = 0.5,         # Minimum spring constant
  k0_max = 30,        # Maximum spring constant 
  cooling_rate_min = 1e-4, # Minimum decay rate
  cooling_rate_max = 0.05, # Maximum decay rate
  c_repulsion_min = 1e-4,    # Minimum repulsion constant
  c_repulsion_max = 0.05     # Maximum repulsion constant
)

# Setup optimization parameters
opt_params <- list(
  mapping_max_iter = 1000,    # Maximum map optimization iterations
  folds = 20,         # Number of CV folds
  num_samples = 50,
  max_cores = NULL,           # Use NULL to let the function decide the number of cores
  write_files = FALSE   # Set TRUE to save intermediate results
)
```

## Initial Parameter Search

We'll run some optimization trials for each dimension and matrix variant to build an initial estimate of likelihood in the parameter space:

```{r eval=FALSE}
# Run optimization for each dimensionality and matrix variant
for(i in seq_along(results$matrices)) { 
  dim_data <- results$matrices[[i]]
  ndim <- results$metadata$dimension[i]
  
  for(matrix_type in matrix_variants) { 
    # Get current matrix
    distance_matrix <- dim_data[[matrix_type]]
    
    # Create scenario name incorporating dimension and matrix type
    scenario_name <- sprintf("synthetic_AMC203_dim%d_%s", ndim, matrix_type)
    
    message(sprintf("\nOptimizing for %d dimensions, matrix type: %s", 
                   ndim, matrix_type))
    
    # Run optimization
    opt_result <- initial_parameter_optimization(
      dissimilarity_matrix = distance_matrix,
      mapping_max_iter = opt_params$mapping_max_iter,
      relative_epsilon = 1e-4,
      convergence_counter = 5,
      scenario_name = scenario_name,
      N_min = param_ranges$N_min, 
      N_max = param_ranges$N_max,
      k0_min = param_ranges$k0_min,
      k0_max = param_ranges$k0_max, 
      c_repulsion_min = param_ranges$c_repulsion_min,
      c_repulsion_max = param_ranges$c_repulsion_max,
      cooling_rate_min = param_ranges$cooling_rate_min,
      cooling_rate_max = param_ranges$cooling_rate_max,
      folds = opt_params$folds,
      num_samples = opt_params$num_samples,
      write_files = opt_params$write_files,
      max_cores = opt_params$max_cores,
      verbose = TRUE
    )
  }
}
```

## Aggregate Results 

After the optimization jobs complete, aggregate the results for each dimension and matrix type:

```{r eval=FALSE}
# Initialize results storage
all_results <- list()

# Aggregate results for each dimension and matrix type
for(ndim in results$metadata$dimension) { 
  dim_results <- list()
  
  for(matrix_type in matrix_variants) { #
    scenario_name <- sprintf("synthetic_AMC203_dim%d_%s", ndim, matrix_type)
    print(scenario_name)
    # Aggregate optimization results
    dim_results[[matrix_type]] <- aggregate_parameter_optimization_results(
      scenario_name = scenario_name,
      write_files = TRUE
    )
  }
  
  all_results[[as.character(ndim)]] <- dim_results
}
```

## Adaptive Monte Carlo Sampling

Perform adaptive sampling for each case.
Beware that each call to run_adaptive_sampling() deletes all the files in adaptive sampling directory and uses this directory for storing its temporary files. Therefore, do not run multiple samplings in one directory concurrently.
You can check the console outputs, warnings, or errors in .out and .err files in /adaptive_sampling_jobs directory.

```{r eval=FALSE}
# Setup adaptive sampling parameters
amc_params <- list(
  num_samples = 200,        # Number of new samples to add to the parameter distribution through AMC (for refinement.)
  mapping_max_iter = 1000,    # Maximum iterations per map optimization
  relative_epsilon = 1e-4
)

# Setup optimization parameters
opt_params <- list(
  folds = 20         # Number of CV folds

)

# Run adaptive sampling for each dimension and matrix type
for(ndim in results$metadata$dimension) { # 
  dim_data <- results$matrices[[which(results$metadata$dimension == ndim)]]
  
  for(matrix_type in matrix_variants) { # 
    # Get scenario names
    base_scenario <- sprintf("synthetic_AMC202_dim%d_%s", ndim, matrix_type)
    amc_scenario <- base_scenario #sprintf("%s_amc", base_scenario)
    
    # Get result file path
    result_file <- file.path("model_parameters", 
                            sprintf("%s_model_parameters.csv", base_scenario))
    log_transform_parameters(result_file)
    
    # Get corresponding distance matrix
    distance_matrix <- dim_data[[matrix_type]]
    
    message(sprintf("\nRunning adaptive sampling for %d dimensions, matrix type: %s", 
                   ndim, matrix_type))
    
    # Run adaptive sampling for current scenario (K-fold cross-validation happens inside) 
    run_adaptive_sampling(
      initial_samples_file = result_file,
      dissimilarity_matrix = distance_matrix,
      mapping_max_iter = amc_params$mapping_max_iter,
      scenario_name = amc_scenario,
      num_samples = amc_params$num_samples,
      max_cores = opt_params$folds+1, # Each job on SLURM can work with 20 cores for a 20-fold CV
      relative_epsilon = amc_params$relative_epsilon,
      folds = opt_params$folds,
      verbose = FALSE
    )
  }
}
```

## Analyze fitted parameters Across Dimensions and Matrix Types

After all jobs complete, analyze parameter variations:

```{r}
# Initialize results collection
final_params <- data.frame()

# Collect results for each dimension and matrix type
for(ndim in results$metadata$dimension) {
  for(matrix_type in matrix_variants) {
    # Define pattern to match multiple AMC result files
    file_pattern <- sprintf("synthetic_AMC\\d+_dim%d_%s_model_parameters\\.csv", 
                           ndim, matrix_type)
    
    # Find all matching files in the model_parameters directory
    amc_files <- list.files(path = "model_parameters", 
                          pattern = file_pattern,
                          full.names = TRUE)
    
    # Check if any files were found
    if(length(amc_files) == 0) {
      cat("Warning: No files found matching pattern", file_pattern, "in model_parameters directory\n")
      next
    }
    
    cat("Found", length(amc_files), "files for dimension", ndim, "and matrix type", matrix_type, ":\n")
    cat(paste(" -", basename(amc_files)), sep = "\n")
    
    # Try to read and combine all matching files
    tryCatch({
      # Combine chain results
      amc_results <- do.call(rbind, lapply(amc_files, function(file) {
        tryCatch({
          df <- read.csv(file)
          # Add source file information for debugging
          df$source_file <- basename(file)
          return(df)
        }, error = function(e) {
          cat("Error reading file", file, ":", e$message, "\n")
          return(NULL)
        })
      }))
      
      # Remove NULL results from failed file reads
      amc_results <- amc_results[!sapply(amc_results, is.null)]
      
      if(nrow(amc_results) == 0) {
        cat("Warning: No valid data found in files for dimension", ndim, "and matrix type", matrix_type, "\n")
        next
      }
      
      # Ensure numeric columns are properly converted
      numeric_columns <- c("log_N", "log_k0", "log_cooling_rate", "log_c_repulsion", "NLL", "Holdout_MAE")
      for(col in numeric_columns) {
        if(col %in% names(amc_results)) {
          amc_results[[col]] <- as.numeric(as.character(amc_results[[col]]))
        }
      }
      
      # Filter valid results
      amc_results <- amc_results %>%
        filter(is.finite(NLL) & is.finite(Holdout_MAE)) %>%
        na.omit()
      
      # Additional filtering for valid dimensions
      amc_results <- amc_results[amc_results$log_N >= log(2),]
      
      # Clean numeric data
      numeric_cols <- sapply(amc_results, is.numeric)
      amc_results[numeric_cols] <- lapply(amc_results[numeric_cols], clean_data, k = 3.5)
      amc_results <- na.omit(amc_results)
      
      # Find best parameters (minimum NLL)
      if(nrow(amc_results) > 0) {
        # Get best parameters by minimum MAE
        best_params <- amc_results[which.min(amc_results$Holdout_MAE),]
        best_params$true_dim <- ndim
        best_params$matrix_type <- matrix_type
        best_params$N <- round(exp(best_params$log_N))
        
        # Also record file source for traceability
        best_params$source_file <- best_params$source_file
        
        final_params <- rbind(final_params, best_params)
        
        cat("Found best parameters for dimension", ndim, "matrix type", matrix_type, 
            "from file", best_params$source_file, ":\n")
        cat("  N =", best_params$N, 
            "k0 =", round(exp(best_params$log_k0), 4),
            "c_repulsion =", formatC(exp(best_params$log_c_repulsion), format = "e", digits = 2),
            "cooling_rate =", formatC(exp(best_params$log_cooling_rate), format = "e", digits = 2),
            "MAE =", round(best_params$Holdout_MAE, 4), "\n\n")
      } else {
        cat("Warning: No valid results after cleaning for dimension", ndim, 
            "and matrix type", matrix_type, "\n\n")
      }
    }, error = function(e) {
      cat("Error processing files for dimension", ndim, "and matrix type", matrix_type, ":", 
          e$message, "\n\n")
    })
  }
}

# Display summary of collected parameters
if(nrow(final_params) > 0) {
  #print(final_params[, c("true_dim", "matrix_type", "N", "Holdout_MAE")])
  
  # Save consolidated results
  results_file <- "optimal_parameters_250points_AMC10X-20X.csv"
  write.csv(final_params, results_file, row.names = FALSE)
  cat("\nSaved consolidated results to", results_file, "\n")
} else {
  cat("No valid parameters found across all dimensions and matrix types.\n")
}


cat("\nSummary of best parameters found:\n")
# Apply the transformation
final_params <- final_params %>%
  mutate(
    Missing = case_when(
      grepl("^S_", matrix_type) ~ "70%",
      grepl("^M_", matrix_type) ~ "85%",
      grepl("^L_", matrix_type) ~ "95%"
    ),
    Variant = case_when(
      grepl("_missing$", matrix_type) ~ "Original",
      grepl("_noise1$", matrix_type) ~ "+Noise",
      grepl("_noise_bias$", matrix_type) ~ "+Noise+Bias"
    )
  ) %>%
  rename("True Dim" = true_dim)

# Create a table with the requested columns and format integers without decimals
library(xtable)
latex_table <- xtable(final_params[, c("Missing", "Variant", "True Dim", "N")],
                      digits = c(0, 0, 0, 0, 0))  # 0 decimal places for all columns
print(latex_table, include.rownames = FALSE)
```

## Convergence Analysis

Check convergence for each case:

```{r}
# A helper function:
analyze_convergence <- function(ndim, matrix_type, mutual_size) {
  # Create the two specific file patterns we want to match
  file_patterns <- c(
    sprintf("synthetic_AMC101_dim%d_%s_model_parameters.csv", ndim, matrix_type),
    sprintf("synthetic_AMC102_dim%d_%s_model_parameters.csv", ndim, matrix_type),
    sprintf("synthetic_AMC201_dim%d_%s_model_parameters.csv", ndim, matrix_type),
    sprintf("synthetic_AMC202_dim%d_%s_model_parameters.csv", ndim, matrix_type),
    sprintf("synthetic_AMC203_dim%d_%s_model_parameters.csv", ndim, matrix_type)
  )

  # Get full paths for both files
  chain_files <- file.path("model_parameters", file_patterns)

  # Verify files exist
  if (!all(file.exists(chain_files))) {
    missing_files <- chain_files[!file.exists(chain_files)]
    warning("Missing chain files: ", paste(missing_files, collapse = ", "))
    chain_files <- chain_files[file.exists(chain_files)]
    if (length(chain_files) == 0) {
      stop("No chain files found for dimension ", ndim, " and matrix type ", matrix_type)
    }
  }

  # Calculate diagnostics
  diag <- calculate_diagnostics(chain_files, mutual_size = mutual_size)

  # Create diagnostic plots
  plot_file <- sprintf("synthetic_dim%d_%s_diagnostics.png", ndim, matrix_type)
  p <- create_diagnostic_plots(chain_files, mutual_size = mutual_size,
                             output_file = plot_file)

  return(list(diagnostics = diag, plot = p))
}

# Set the mutual_size, smaller then the minimum simulaations for each scenario:
mutual_size <- 400

# Analyze convergence for each case
convergence_results <- list()
for(ndim in results$metadata$dimension) {
  dim_results <- list()
  for(matrix_type in matrix_variants) {
    dim_results[[matrix_type]] <- analyze_convergence(ndim, matrix_type, mutual_size=mutual_size)
  }
}

# Print summary of convergence results
for(ndim in names(convergence_results)) {
  cat(sprintf("\nConvergence Summary for %s dimensions:\n", ndim))
  for(matrix_type in names(convergence_results[[ndim]])) {
    cat(sprintf("\nMatrix type: %s\n", matrix_type))
    print(convergence_results[[ndim]][[matrix_type]]$diagnostics)
  }
}
```

## Session Info

```{r}
sessionInfo()
```
