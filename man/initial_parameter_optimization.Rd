% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adaptive_sampling.R
\name{initial_parameter_optimization}
\alias{initial_parameter_optimization}
\title{Parameter Space Sampling and Optimization Functions for topolow}
\usage{
initial_parameter_optimization(
  dissimilarity_matrix,
  mapping_max_iter = 20,
  relative_epsilon,
  convergence_counter,
  scenario_name,
  N_min,
  N_max,
  k0_min,
  k0_max,
  c_repulsion_min,
  c_repulsion_max,
  cooling_rate_min,
  cooling_rate_max,
  num_samples = 20,
  epochs = 1,
  max_cores = NULL,
  folds = 20,
  opt_subsample = NULL,
  verbose = FALSE,
  write_files = FALSE,
  output_dir
)
}
\arguments{
\item{dissimilarity_matrix}{Matrix. Input dissimilarity matrix. Must be square and symmetric.}

\item{mapping_max_iter}{Integer. Maximum number of optimization iterations for each map.}

\item{relative_epsilon}{Numeric. Convergence threshold for relative change in error.}

\item{convergence_counter}{Integer. Number of iterations below threshold before declaring convergence.}

\item{scenario_name}{Character. Name for output files and job identification.}

\item{N_min, N_max}{Integer. Range for the number of dimensions parameter.}

\item{k0_min, k0_max}{Numeric. Range for the initial spring constant parameter.}

\item{c_repulsion_min, c_repulsion_max}{Numeric. Range for the repulsion constant parameter.}

\item{cooling_rate_min, cooling_rate_max}{Numeric. Range for the cooling rate parameter.}

\item{num_samples}{Integer. Number of LHS samples to generate \strong{per epoch}. Default: 20.}

\item{epochs}{Integer. Number of optimization epochs. In each epoch, parameters are sampled,
evaluated, and the best 50\% are used to refine the search space for the next epoch.
Default: 3.}

\item{max_cores}{Integer. Maximum number of cores for parallel processing. Default: NULL (uses all but one).}

\item{folds}{Integer. Number of cross-validation folds. Default: 20.}

\item{opt_subsample}{Integer or NULL. If specified, randomly samples this many
points from the dissimilarity matrix for each parameter evaluation to reduce
computational cost. The function automatically validates that subsampled data
forms a connected graph. Each parameter evaluation uses a different random
subsample for robustness. Default: NULL (use full data).

\strong{Notes:}
\itemize{
\item If connectivity cannot be achieved, sample size is adaptively increased
\item Minimum recommended: \code{opt_subsample >= max(100, folds)}
\item Each parameter set gets a different subsample, but all CV folds within
that parameter set use the same subsample
\item The actual subsample size used is reported in output columns
}}

\item{verbose}{Logical. Whether to print progress messages. Default: FALSE.}

\item{write_files}{Logical. Whether to save results to a CSV file. Default: FALSE.}

\item{output_dir}{Character. Directory for output files. Required if \code{write_files} is TRUE.}
}
\value{
A \code{data.frame} containing the log-transformed parameter sets and their performance metrics
from the \strong{final epoch}. Columns include: \code{log_N}, \code{log_k0}, \code{log_cooling_rate}, \code{log_c_repulsion},
\code{Holdout_MAE}, \code{NLL}, and if \code{opt_subsample} was used: \code{opt_subsample},\code{original_n_points}.
}
\description{
Performs parameter optimization using Latin Hypercube Sampling (LHS) combined with
k-fold cross-validation. Parameters are sampled from specified ranges using maximin
LHS design to ensure good coverage of parameter space. Each parameter set is evaluated
using k-fold cross-validation to assess prediction accuracy. To calculate one NLL per set of
parameters, the function uses a pooled errors approach which combine all validation errors into
one set, then calculate a single NLL. This approach has two main advantages:
1- It treats all validation errors equally, respecting the underlying error distribution assumption
2- It properly accounts for the total number of validation points

\strong{Note}: As of version 2.0.0, this function returns log-transformed parameters directly,
eliminating the need to call \code{log_transform_parameters()} separately.
}
\details{
Initial Parameter Optimization using Latin Hypercube Sampling

The function performs these steps in an epoch-based evolutionary strategy:
\enumerate{
\item \strong{Initialization}: Starts with the user-provided parameter ranges.
\item \strong{Epoch Loop}: For each epoch:
a. Generates \code{num_samples} using LHS within the current parameter ranges.
b. If \code{opt_subsample} is specified, each evaluation uses a random subsample.
c. Evaluates parameter sets via cross-validation (in parallel batches).
d. \strong{Range Update} (after all but the final epoch):
\itemize{
\item Sorts results by NLL and keeps the top 50\%.
\item Updates parameter ranges for the next epoch based on survivors:
New Min = 0.75 * Min(Survivors), New Max = 1.25 * Max(Survivors).
\item This allows the search to drift and zoom in on optimal regions.
}
\item \strong{Finalization}: Automatically log-transforms the results from the \strong{final epoch}
for direct use with adaptive sampling.
}

\strong{Note on Cross-Validation with Subsampling:}
When \code{opt_subsample} is used, each parameter evaluation receives a different
random subsample. Since the underlying data differs between evaluations, each evaluation
also gets its own CV fold structure (created internally by \code{likelihood_function}).
This approach tests parameter robustness across different data samples and fold structures,
which is appropriate for the exploration phase of parameter optimization.
}
\note{
\strong{Breaking Change in v2.0.0:} This function now returns log-transformed parameters directly.
The returned data frame has columns \code{log_N}, \code{log_k0}, \code{log_cooling_rate}, \code{log_c_repulsion}
instead of the original scale parameters. This eliminates the need to call \code{log_transform_parameters()}
separately before using \code{run_adaptive_sampling()}.

\strong{Breaking Change in v2.0.0:} The parameter \code{distance_matrix} has been renamed to
\code{dissimilarity_matrix}. Please update your code accordingly.
}
\examples{
\donttest{
# This example can exceed 5 seconds on some systems.
# 1. Create a simple synthetic dataset for the example
synth_coords <- matrix(rnorm(60), nrow = 20, ncol = 3)
dist_mat <- coordinates_to_matrix(synth_coords)

# 2. Run the optimization on the synthetic data (full data)
results <- initial_parameter_optimization(
  dissimilarity_matrix = dist_mat,
  mapping_max_iter = 100,
  relative_epsilon = 1e-3,
  convergence_counter = 2,
  scenario_name = "test_opt_synthetic",
  N_min = 2, N_max = 5,
  k0_min = 1, k0_max = 10,
  c_repulsion_min = 0.001, c_repulsion_max = 0.05,
  cooling_rate_min = 0.001, cooling_rate_max = 0.02,
  num_samples = 4,
  epochs = 2, # Use 2 epochs
  max_cores = 1,
  verbose = FALSE
)

# 3. With subsampling for faster computation
results_sub <- initial_parameter_optimization(
  dissimilarity_matrix = dist_mat,
  mapping_max_iter = 100,
  relative_epsilon = 1e-3,
  convergence_counter = 2,
  scenario_name = "test_opt_subsampled",
  N_min = 2, N_max = 5,
  k0_min = 1, k0_max = 10,
  c_repulsion_min = 0.001, c_repulsion_max = 0.05,
  cooling_rate_min = 0.001, cooling_rate_max = 0.02,
  num_samples = 4,
  epochs = 1,
  max_cores = 1,
  folds = 10,
  opt_subsample = 15,  # Use only 15 points
  verbose = TRUE
)
}

}
\seealso{
\code{\link{euclidean_embedding}} for the core optimization algorithm,
\code{\link{subsample_dissimilarity_matrix}} for subsampling details.
}
