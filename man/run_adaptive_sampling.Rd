% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adaptive_sampling.R
\name{run_adaptive_sampling}
\alias{run_adaptive_sampling}
\title{Run Adaptive Monte Carlo Sampling}
\usage{
run_adaptive_sampling(
  initial_samples_file,
  distance_matrix,
  num_parallel_jobs = 5,
  max_cores = NULL,
  num_samples = 10,
  mapping_max_iter,
  relative_epsilon = 1e-04,
  folds = 20,
  time = "8:00:00",
  memory = "10G",
  scenario_name,
  output_dir = NULL,
  use_slurm = FALSE,
  cider = FALSE,
  verbose = FALSE
)
}
\arguments{
\item{initial_samples_file}{Character. Path to CSV file containing initial samples.
Must contain columns: log_N, log_k0, log_cooling_rate, log_c_repulsion, NLL}

\item{distance_matrix}{Matrix. Distance matrix to optimize.}

\item{num_parallel_jobs}{Integer. Number of parallel jobs (cores on local machine or SLURM jobs).}

\item{max_cores}{Integer. Maximum number of cores to use for parallel processing. If NULL,
uses all available cores minus 1 (default: NULL).}

\item{num_samples}{Integer. Number of new samples to be added to the CSV file containing initial samples through Adaptive Monte Carlo sampling (default: 10).}

\item{mapping_max_iter}{Integer. Maximum iterations per map optimization.}

\item{relative_epsilon}{Numeric. Convergence threshold.}

\item{folds}{Integer. Number of CV folds (default: 10).}

\item{time}{Character. Walltime for SLURM jobs in HH:MM:SS format. Default: "8:00:00".}

\item{memory}{Character. Memory allocation for SLURM jobs. Default: "10G".}

\item{scenario_name}{Character. Name for output files.}

\item{output_dir}{Character. Directory for output files. If NULL, uses current directory.}

\item{use_slurm}{Logical. Whether to use SLURM (default: FALSE).}

\item{cider}{Logical. Whether to use cider queue (default: FALSE).}

\item{verbose}{Logical. Whether to print progress messages. Default: FALSE.}
}
\value{
Invisible NULL. Results are appended to:
model_parameters/\{scenario_name\}_model_parameters.csv
}
\description{
Performs adaptive Monte Carlo sampling to explore parameter space, either locally
or distributed via SLURM. Samples are drawn adaptively based on previous evaluations
to focus sampling in high-likelihood regions. Results from all jobs accumulate in
a single output file.
}
\details{
The function:
\enumerate{
\item Takes initial parameter samples as starting points
\item Creates \code{num_parallel_jobs} independent sampling processes
\item Updates sampling distribution based on likelihoods
\item Can distribute computation via SLURM for large-scale sampling
}

Workflow:
\enumerate{
\item Runs/Submits 'num_parallel_jobs' independent single-core jobs
\item Each job runs for num_samples/num_parallel_jobs iterations
\item Each job adds samples to a common results file
\item Total: Adds num_samples samples
}

Both local and SLURM executions append results to the same output file:
model_parameters/\{scenario_name\}_model_parameters.csv
}
\examples{
\dontrun{
# Read initial samples
init_file <- "initial_samples.csv"

# Create distance matrix
dist_mat <- matrix(runif(100), 10, 10)
dist_mat[lower.tri(dist_mat)] <- t(dist_mat)[lower.tri(dist_mat)]
diag(dist_mat) <- 0

# Run local adaptive sampling with 4 parallel jobs
run_adaptive_sampling(
  initial_samples_file = init_file,
  distance_matrix = dist_mat,
  mapping_max_iter = 1000,
  scenario_name = "test_sampling",
  num_parallel_jobs = 4,
  max_cores = 4,
  num_samples = 8,
  verbose = TRUE
)

# Run with SLURM using 50 parallel jobs
run_adaptive_sampling(
  initial_samples_file = init_file, 
  distance_matrix = dist_mat,
  scenario_name = "slurm_sampling",
  num_parallel_jobs = 50,
  use_slurm = TRUE
)
}

}
\seealso{
\code{\link{adaptive_MC_sampling}} for the core sampling algorithm
}
