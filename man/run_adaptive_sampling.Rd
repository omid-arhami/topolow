% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adaptive_sampling.R
\name{run_adaptive_sampling}
\alias{run_adaptive_sampling}
\title{Run Adaptive Monte Carlo Sampling}
\usage{
run_adaptive_sampling(
  initial_samples_file,
  distance_matrix,
  parallel_jobs = 5,
  iterations = 5,
  max_iter,
  relative_epsilon = 1e-04,
  folds = 20,
  time = "8:00:00",
  memory = "10G",
  scenario_name,
  output_dir = NULL,
  use_slurm = FALSE,
  cider = FALSE,
  verbose = FALSE
)
}
\arguments{
\item{initial_samples_file}{Character. Path to CSV file containing initial samples.
Must contain columns: log_N, log_k0, log_cooling_rate, log_c_repulsion, NLL}

\item{distance_matrix}{Matrix. Distance matrix to optimize.}

\item{parallel_jobs}{Integer. Number of parallel jobs (cores on local machine or SLURM jobs).}

\item{iterations}{Integer. Number of sampling iterations per job (default: 1).}

\item{max_iter}{Integer. Maximum iterations per optimization.}

\item{relative_epsilon}{Numeric. Convergence threshold.}

\item{folds}{Integer. Number of CV folds (default: 10).}

\item{time}{Character. Walltime for SLURM jobs in HH:MM:SS format. Default: "8:00:00".}

\item{memory}{Character. Memory allocation for SLURM jobs. Default: "10G".}

\item{scenario_name}{Character. Name for output files.}

\item{output_dir}{Character. Directory for output files. If NULL, uses current directory.}

\item{use_slurm}{Logical. Whether to use SLURM (default: FALSE).}

\item{cider}{Logical. Whether to use cider queue (default: FALSE).}

\item{verbose}{Logical. Whether to print progress messages. Default: FALSE.}
}
\value{
Invisible NULL. Results are appended to:
model_parameters/\{scenario_name\}_model_parameters.csv
}
\description{
Performs adaptive Monte Carlo sampling to explore parameter space, either locally
or distributed via SLURM. Samples are drawn adaptively based on previous evaluations
to focus sampling in high-likelihood regions. Results from all jobs accumulate in
a single output file.
}
\details{
The function:
\enumerate{
\item Takes initial parameter samples as starting points
\item Creates iterations of sampling for each parallel job
\item Updates sampling distribution based on likelihoods
\item Can distribute computation via SLURM for large-scale sampling
}

Both local and SLURM executions append results to the same output file:
model_parameters/\{scenario_name\}_model_parameters.csv
}
\examples{
\dontrun{
# Read initial samples
init_file <- "initial_samples.csv"

# Create distance matrix
dist_mat <- matrix(runif(100), 10, 10)
dist_mat[lower.tri(dist_mat)] <- t(dist_mat)[lower.tri(dist_mat)]
diag(dist_mat) <- 0

# Run local adaptive sampling with 4 parallel jobs
run_adaptive_sampling(
  initial_samples_file = init_file,
  distance_matrix = dist_mat,
  max_iter = 1000,
  scenario_name = "test_sampling",
  parallel_jobs = 4,
  iterations = 5,
  verbose = TRUE
)

# Run with SLURM using 50 parallel jobs
run_adaptive_sampling(
  initial_samples_file = init_file, 
  distance_matrix = dist_mat,
  scenario_name = "slurm_sampling",
  parallel_jobs = 50,
  use_slurm = TRUE
)
}

}
\seealso{
\code{\link{adaptive_MC_sampling}} for the core sampling algorithm
}
